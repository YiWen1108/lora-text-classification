# LoRA Text Classification (æƒ…ç·’åˆ†é¡)

This project demonstrates how to fine-tune a DistilBERT model using **LoRA (Low-Rank Adaptation)** for binary sentiment classification using the IMDb dataset.

æœ¬å°ˆæ¡ˆå±•ç¤ºå¦‚ä½•ä½¿ç”¨ LoRA å¾®èª¿ DistilBERT æ¨¡å‹ï¼Œå¯¦ä½œ IMDb é›»å½±è©•è«–çš„æƒ…ç·’äºŒåˆ†é¡ä»»å‹™ã€‚

---

## ğŸ“¦ Requirements (å®‰è£éœ€æ±‚)

```bash
pip install -r requirements.txt
```

---

## ğŸš€ How to Run (å¦‚ä½•åŸ·è¡Œ)

### Step 1: Train the model
```bash
python lora_finetune.py
```

### Step 2: Run sentiment prediction
```bash
python evaluate.py
```

---

## ğŸ§  Model

- Base model: `distilbert-base-uncased`
- Fine-tuning method: LoRA (via PEFT library)
- Dataset: IMDb (2000 samples)

---

## ğŸ“ Output example

```json
[
  {"label": "POSITIVE", "score": 0.9998},
  {"label": "NEGATIVE", "score": 0.9983}
]
```

---

## ğŸ’¡ Tip

This is a lightweight and fast training example. You can extend it to other tasks or datasets easily.

é€™æ˜¯ä¸€å€‹ç°¡æ½”å¿«é€Ÿçš„è¨“ç·´ç¯„ä¾‹ï¼Œé©åˆç”¨ä¾†å­¸ç¿’ LoRA å¾®èª¿ï¼Œå¯é€²ä¸€æ­¥æ‡‰ç”¨åˆ°å…¶ä»–ä»»å‹™æˆ–è³‡æ–™é›†ã€‚
